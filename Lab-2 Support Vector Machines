{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SD-TSIA 211 :  Lab 2 Séparateurs à Vaste Marge\n",
    "\n",
    "## Julie Keisler - Aurore Gosmant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part I - Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_breastcancer(filename):\n",
    "    \"\"\"\n",
    "    This function reads a file, for instance : \n",
    "    filename = 'wdbc_M1_B0.data'\n",
    "    It returns : \n",
    "    X : matrix of caracteristics\n",
    "    y : a vector of classes such as :\n",
    "        y[i] = 1, la tumeur est maligne, and if\n",
    "        y[i] = -1, la tumeur est benigne\n",
    "\n",
    "    For more information on the database : \n",
    "    https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+%28Prognostic%29\n",
    "    \"\"\"\n",
    "\n",
    "    data = np.loadtxt(filename, delimiter=',')\n",
    "\n",
    "    # the column 0 doesn't interest us here\n",
    "    y = data[:, 1] * 2 - 1\n",
    "    X = data[:, 2:]\n",
    "\n",
    "    # Standardisation of the matrix\n",
    "    X = X - np.mean(X, axis=0)\n",
    "    X = X / np.std(X, axis=0)\n",
    "\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,Y = load_breastcancer('wdbcM1B0.data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part II - Subgradient method\n",
    "\n",
    "### Question 2.1\n",
    "\n",
    "Let's solve the following problem : \n",
    "$$\n",
    "\\min_{v \\in \\mathbb{R}^m, a \\in \\mathbb{R}, \\xi \\in \\mathbb{R}^n} \\frac{1}{2}  \\sum_{j = 1}^{m} v_{j}^{2} + c\\sum_{j = 1}^{n} \\xi_{i} \n",
    "$$ by showing it is equivalent to solving : \n",
    "\n",
    "$$  \\\\ min_{v \\in \\mathbb{R}^m} \\frac{1}{2}  \\sum_{j = 1}^{m} v_{j}^{2} + c\\sum_{j = 1}^{n} \\max(0, 1 - y_{i}(x_{i}^{T}v + a)). \\\\ $$\n",
    "\n",
    "\n",
    "Let's define the set $\\Xi (v,a) = [max(0, 1-y_i(x_i^{T} + a), +\\infty[ = \\prod_{i} \\Xi_{i}$\n",
    "\n",
    "Let $f(v,a,\\xi) = \\frac{1}{2} \\sum_{j=1}{m} v_j^{2} + c\\sum_{j=1}^{m}\\xi_{i}$\n",
    "\n",
    "Let $\\xi_{i} \\in \\Xi (v,a)$.\n",
    "\n",
    "f is an increasing function of $\\xi$ on $\\Xi (v,a)$.\n",
    "\n",
    "Let $\\xi_{min} = \\prod_{i} (max(0, 1-y_i(x_i^{T}v + a)).$\n",
    "\n",
    "We f have that  : $\\forall \\xi$ in $\\Xi : f(v,a,\\xi) \\ge f(v,a,\\xi_{min}). $ \n",
    "\n",
    "Because f is a function of multiple variables, therefore, we can deduce that :\n",
    "\n",
    "$\\forall \\xi$ in $\\Xi, : f(v,a,\\xi) \\le f(v,a,\\xi_{min})$\n",
    "\n",
    "The two problems are then equal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $ h : \\mathbb{R} \\rightarrow \\mathbb{R}$ such that $h(z) = max(0, 1-z). $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXIAAAD4CAYAAADxeG0DAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjAsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+17YcXAAAgAElEQVR4nO3de5zOdf7/8cdrxmkcSsYkmYp00ipiklaGTJhiJbFU39oOcvtuqVS02iwK3dSkVNu2sXLKhpJz1iGH2ukbDUYiqazKosah1laSvH9/vJmfHGfM57o+c13X8367uTVzzWfen9c18vTx/rzfr4855xARkdiVFHYBIiJSMgpyEZEYpyAXEYlxCnIRkRinIBcRiXFlwjhp9erVXe3atcM4tYhIzFq+fPk251zaoa+HEuS1a9cmLy8vjFOLiMQsM/v8SK9rakVEJMYpyEVEYpyCXEQkxoUyRy4iRfPTTz+xadMmdu/eHXYpEkUVKlQgPT2dsmXLFul4BblIKbZp0yaqVKlC7dq1MbOwy5EocM6xfft2Nm3aRJ06dYr0PYFMrZhZVTN73czWmdlHZnZ5EOOKJLrdu3eTmpqqEE8gZkZqamqx/hUW1BX5s8A/nHOdzawcUDGgcUUSnkI88RT397zEV+RmdhKQCYwCcM7tcc59U9Jxj2jpUsjJAbXeFREpFMTUytlAATDazFaa2d/MrNKhB5lZDzPLM7O8goKCEzvTuHHw0EPQp4/CXERkvyCCvAzQCHjROXcJ8B3Q99CDnHMjnHMZzrmMtLTDdpgWzfPPQ8+eMGwYdO8Oe/eWpG4ROY6NGzdSv379o369V69evP3220f9eu/evVm4cGEkSgvM8OHDGTduHAA7duygdevWnHvuubRu3ZqdO3ce9/uzs7OpWrUq7du3L9Z59+zZQ2ZmJnsDyLEggnwTsMk5t3T/56/jgz14SUnw3HPQvz+8/DL89regZVkiodixYwfvvfcemZmZRz3mnnvuYejQoVGsqnj27t3Lyy+/zI033gjA0KFDycrK4pNPPiErK6tItffp04fx48cX+9zlypUjKyuLSZMmFft7D1Xim53Oua1m9qWZne+c+xjIAtaWuLKjMYNHH4Vq1aBXL2jfHqZOhSpVInZKkVKhVy/Izw92zIYNYfjwYx7y888/c+edd/Luu+9Sq1Ytpk+fTkpKCq+//jrZ2dkA5OXl0b1798LjP/zwQ5xznHXWWWzfvp2tW7dy2mmnnXCZixcvZsCAAdSoUYP8/Hw6derERRddxLPPPssPP/zAtGnTqFu3LjNnzmTw4MHs2bOH1NRUJkyYQI0aNbj33nupXr06/fv3Z+7cuQwZMoTFixezcOFCGjVqRJkyPgqnT5/O4sWLAfjd735Hy5YteeKJJ45ZW1ZWVuH3HE3//v2ZMWMGAAUFBbRp04bRo0fTsWNHHn74YW666aYT/tlAcDs77wEmmNkHQEPg8YDGPbr77oOxY2HxYrjqKti+PeKnFElEn3zyCXfffTdr1qyhatWqTJkyBYDc3FwaN24MQEZGBvn5+eTn55OdnU3v3r0Lv79Ro0bk5uYeNm5OTg4NGzY87Ne99957xDpWrVrFs88+y+rVqxk/fjzr169n2bJldO/eneeffx6AK664gvfee4+VK1fSrVs3nnzyScBfaU+aNIlFixZx7733Mnr0aJKSkn7xHgC++uoratasCUDNmjX5+uuvA/gJwmOPPUZ+fj5LliwhNTWVnj17AlC/fn3ef//9Eo8fyPJD51w+kBHEWMVyyy1QtaqfYsnMhHnzoFatqJchEhXHuXKOlDp16tCwYUMAGjduzMaNGwHYsmULh97vmjx5MitWrGDevHmFr5166qls3rz5sHH79OlDnz59ilzHpZdeWhiydevWpU2bNgBcdNFFLFq0CPAbqLp27cqWLVvYs2dP4YaaihUrMnLkSDIzM3nmmWeoW7du4XuoV69ekWsoCeccN910E/fff3/hXx7JycmUK1eOXbt2UaUEswqx32ulQweYOxe+/BKaNYNPPgm7IpG4Ur58+cKPk5OTC2/OpaSk/GLTypo1axgwYAATJ04kOTm58PXdu3eTkpJy2LjFvSI/uI6kpKTCz5OSkgpruueee+jZsyerV6/mpZde+kV9q1evJjU19Rd/qRz6HmrUqMGWLVsAH/KnnnpqEX5Ch1u6dGnh+zkwpTJw4EDS09O57bbbfnHsjz/+SIUKFU7oPAfEfpADtGgBixbBd9/BFVcEP48oIoepV68en376KQDffvst3bp1Y9y4cYddpa9fv/6IK1/69OlTOB1z8K/nnnvuhGv69ttvqbX/X+Vjx44tfP3zzz9n2LBhrFy5kjlz5rB06dLD3gNAhw4dCr9v7NixXHvttQAsW7aMW265pch1XHbZZYXvp0OHDsyaNYv58+cf9t62b99OWlpakXuqHE18BDlA48bwzjtQvjy0bAn//GfYFYnEtXbt2hXe5Js2bRqff/45d955Z+GVKPimX59++ikZGdGZeR04cCBdunShefPmVK9eHfBTGnfccQdPPfUUp59+OqNGjaJ79+7s3r2bq6+++hfLJ/v27cv8+fM599xzmT9/Pn37+pXUX3zxxRH/VQHQvHlzunTpwltvvUV6ejpz58497Jhhw4axefNmmjRpQsOGDenfvz8AixYt4pprrin5G3fORf1X48aNXcR8/rlz553nXEqKc7NnR+48IlGwdu3asEs4pmbNmrmdO3ce9etvvPGG69evXxQrKr6OHTu69evXH/OY3r17u1WrVgV+7uuuu86tW7fuiF870u89kOeOkKnxc0V+wJln+ivzevXg2mvh1VfDrkgkbg0bNowvvvjiqF/fu3cvDz74YBQrKr6hQ4cWzosfTU5ODhdffHGg592zZw8dO3bk/PPPL/FY5kLY6p6RkeEi/szO//zH3wh9+2144QX4/e8jez6RCPjoo4+itqpCSpcj/d6b2XLn3GHzVPF3RX7ASSfBnDl+w9Bdd8GQIerPIiJxKX6DHCAlBaZMgZtvhn794MEHYd++sKsSEQlU/D8hqGxZGDMGTjkFnnkGdu6EkSOhTPy/dRFJDImRZklJfldcaioMGADffONvgpZwEb6ISGkQ31MrBzPzXROffx6mTYNrroFdu8KuSiSmDBw4kKeeeqrIx8+YMeOEux9OmzaNtWv/f/+9/v37s2DBghMaC6Bz585s2LABgMqVKx/xmD//+c+MHj36hM8RlsQJ8gN69oRXXvGrWVq1gm3bwq5IJC7t3buXDh06FG6qKa5Dg/yxxx7jqquuOqGx1qxZw88//8zZZ599zONuv/32Eu0sDUtiTK0c6qab4OSToUsXaN7cN9s644ywqxI5ppC62DJkyBDGjRvHGWecQVpaGo0bN+azzz7j7rvvpqCgoLAh1QUXXMCtt95KtWrVWLlyJY0aNeKiiy4iLy+PIUOG0KBBAzZs2EBSUhLff/89559/Phs2bGDMmDGMGDGCPXv2cM455zB+/Hjy8/OZMWMGS5YsYfDgwUyZMoVBgwbRvn17KlWqxOjRo5k8eTLgW9wOGzaMmTNnMm/ePAYMGMCPP/5I3bp1GT16NJUrV2bChAmF2+0PeOSRR5g1axYpKSlMnz6dGjVqULFiRWrXrs2yZcto0qRJsD/sCEq8K/ID2rf3zbY2b/b9WdavD7sikVJn+fLlTJw4kZUrV/LGG28Utlzt0aMHzz//PMuXL+epp57irrvuKvye9evXs2DBAoYNG1b42sknn0yDBg1YsmQJADNnzqRt27aULVuWTp068f7777Nq1Srq1avHqFGj+PWvf02HDh3IyckhPz+/sFshQOvWrXnvvff47rvvAJg0aRJdu3Zl27ZtDB48mAULFrBixQoyMjJ4+umnAQ5rV/vdd9/RtGlTVq1aRWZmJiNHjiz8WkZGBu+8804EfpqRk5hX5AdkZvp+5m3b+jD/xz+gUWQebiRSUmF0sX3nnXe47rrrqFixIuCbSu3evZt3332XLl26FB73448/Fn7cpUuXX3Q/PKBr165MmjSJK6+8kokTJxaG/4cffki/fv345ptv+O9//0vbtm2PWVOZMmXIzs5m5syZdO7cmdmzZ/Pkk0+yZMkS1q5dS7NmzQC/c/Lyyy8HDm+5W65cucJHszVu3Jj58+cXfu3UU09l3bp1xfo5hS2xgxzgkkt8g63WreHKK2HmTB/wIgKAmf3i83379lG1alXyjzLPU6nSYc9eB/xfAg8//DA7duxg+fLltGrVCoBbb72VadOm0aBBA8aMGXPcp+2A/0vhhRdeoFq1alx66aVUqVIF5xytW7fm1SO05Ti0XW3ZsmUL39fBrXnh6G13S7PEnVo52HnnQW4unH66vzqfNSvsikRKhczMTKZOncoPP/zArl27mDlzJhUrVqROnTq89tprgG+8t2rVquOOVblyZZo0acJ9991H+/btC6/ad+3aRc2aNfnpp5+YMGFC4fFVqlRh11FWlrVs2ZIVK1YwcuRIunbtCkDTpk3Jzc0tbEv7/fffs37/lOmh7WqP5Whtd0szBfkB6em+2Vb9+tCxIxz0P5RIomrUqBFdu3alYcOGXH/99TRv3hyACRMmMGrUKBo0aMCvfvUrpk+fXqTxunbtyiuvvFIYvgCDBg3isssuo3Xr1lxwwQWFr3fr1o2cnBwuueQSPvvss1+Mk5ycTPv27ZkzZ07hFElaWhpjxozhhhtu4OKLL6Zp06aFUyQHt9w9ntzc3BNeHROW+G2adaJ27fJdExct8mvO9z9bTyQMapoVjB9++IErr7yS3NzcI87fH7By5Uqefvppxo8fH8XqjizqTbPMbKOZrTazfDMrpQldRFWqwJtv+jC/5x547DE12xKJcSkpKTz66KP8+9//PuZx27ZtY9CgQVGqKjhB3uy80jkXH7trKlSA11+HO+/0W/q3b/d9WpI0EyXR55w77IajFN/xVsOAX9pYGhR3pkSrVo6mTBkYNeqXzbZGjfJNuESipEKFCmzfvp3U1FSFeYJwzrF9+/ZiPZA5qCB3wDwzc8BLzrkRhx5gZj2AHgBnnnlmQKeNsKQkGDbMN9vq188325o0ybfHFYmC9PR0Nm3aREFBQdilSBRVqFCB9PT0Ih8fyM1OMzvdObfZzE4F5gP3OOfePtrxpfpm59H85S/+xmdmJsyY4R9cISISRRG92emc27z/v18DU4HYaVJQVHfd5Zck5ub6jUO6QhKRUqLEQW5mlcysyoGPgTbAhyUdt1S64QaYPh3WrvXNto7x0FkRkWgJ4oq8BvBPM1sFLANmO+f+EcC4pdM118D8+bB1KzRrBjHWk0FE4k+Jg9w5t8E512D/r18554YEUVipdsUVvtnWnj3+ynz58rArEpEEpoXRJ6phQ99sq1IlP2dexO2/IiJBU5CXxLnn+pufZ5wB2dl+NYuISJQpyEuqVi3/2LgGDaBTJxg3LuyKRCTBKMiDkJoKCxZAixbwu9/Bs8+GXZGIJBAFeVCqVIHZs+G66/zDFfv3V7MtEYkKBXmQKlSAyZPhtttg0CDfPXHfvrCrEpE4p6ZZQTvQbKtaNd+nZedOGDNGzbZEJGIU5JFgBjk5PswfeQS+/dZfqe9/gK2ISJA0tRIpZvDHP8KLL/oHVbRt67sniogETEEeaf/7v/Dqq/Dee37j0FdfhV2RiMQZBXk0dO0KM2fCxx/77f0bN4ZdkYjEEQV5tGRn+2Zb27b5MF+7NuyKRCROKMijqVkzWLIEfv7ZN9t6//2wKxKROKAgj7aLL/bNtk4+GVq1goULw65IRGKcgjwMdev6MD/rLLj6apg2LeyKRCSGKcjDcvrpvtlWo0Zw/fUwenTYFYlIjFKQh6laNX8DNCsLbr8dnnkm7IpEJAYpyMNWubJfmti5MzzwAPTrp2ZbIlIsCvLSoHx5mDgR7rwThgyBu+9Wsy0RKTL1WiktkpPhpZf8dMsTT/hmW2PHQrlyYVcmIqVcYEFuZslAHvBv51z7oMZNKGYwdKgP8z/8wTfbev11NdsSkWMKcmrlPuCjAMdLXA89BCNHwty50KaNmm2JyDEFEuRmlg60A/4WxHgCdO8OkybBsmXQsiVs3Rp2RSJSSgV1RT4ceAg46h06M+thZnlmlldQUBDQaeNc584waxZ88onvz/Kvf4VdkYiUQiUOcjNrD3ztnFt+rOOccyOccxnOuYy0tLSSnjZxtGkDb70FO3b4MF+zJuyKRKSUCeKKvBnQwcw2AhOBVmb2SgDjygFNm/pdoM5BZiYsXRp2RSJSipQ4yJ1zDzvn0p1ztYFuwELn3P+UuDL5pfr1fX+WqlX9TtAFC8KuSERKCW0IiiVnn+3D/OyzoV07mDIl7IpEpBQINMidc4u1hjzCatb0Pc0bN4bf/hZGjQq7IhEJma7IY9Epp/hmW61b+2WKOTlhVyQiIVKQx6pKlWDGDP880IcegocfVrMtkQSlXiuxrFw5mDDB3wAdOtT3Z3nhBd+3RUQShoI81iUnw4svQmoqPP64D/Px49VsSySBKMjjgZlvf1utGvTu7ZttTZnip19EJO5pjjyePPigX8Vy4Ebozp1hVyQiUaAgjze33w6vvQbLl0OLFrBlS9gViUiEKcjjUadO8OabsGGD78+yYUPYFYlIBCnI41VWlm+29c03PsxXrw67IhGJEAV5PLvsMt9sy8w32/q//wu7IhGJAAV5vPvVryA3F6pXh6uugnnzwq5IRAKmIE8EtWvDO+/AOedA+/b+ZqiIxA0FeaI47TTfbKtJE7+tf+TIsCsSkYAoyBNJ1ap+aiU7G3r0gCeeCLsiEQmAgjzRVKwI06ZBt27Qt69vuKVmWyIxTVv0E1G5cvDKK74dbk6Ofx7oSy+p2ZZIjFKQJ6rkZN8pMTUVBg/2680nTIDy5cOuTESKSUGeyMxg0CDfbOuBB3yzralToXLlsCsTkWLQHLnA/ffD6NGwcKFfa759e9gViUgxlDjIzayCmS0zs1VmtsbMHg2iMImyW2/1rW9XrvTNtjZvDrsiESmiIK7IfwRaOecaAA2BbDNrGsC4Em0dO8KcOfD559CsGXz6adgViUgRlDjInfff/Z+W3f9L69liVatWfopl1y7fbOuDD8KuSESOI5A5cjNLNrN84GtgvnNu6RGO6WFmeWaWV1BQEMRpJVIuvdRv6S9b1jfbys0NuyIROYZAgtw597NzriGQDjQxs/pHOGaEcy7DOZeRlpYWxGklkurV8wFeo4Z/2tA//hF2RSJyFIGuWnHOfQMsBrKDHFdCcuaZ/sr8ggvgN7+BiRPDrkhEjiCIVStpZlZ1/8cpwFXAupKOK6XEqafCokXw61/DjTfCX/8adkUicoggrshrAovM7APgffwc+awAxpXS4uST/dRKu3bw+9/D44+rP4tIKVLinZ3OuQ+ASwKoRUqzlBR44w247TZ45BHfnyUnx+8OFZFQaYu+FF3ZsjBunN/SP2yYD/MRI6CM/jcSCZP+BErxJCXBs8/6MH/0Ud9s6+9/hwoVwq5MJGGp14oUnxkMHAjDh/smW+3a+Q1EIhIKBbmcuPvug7Fj/SPksrLUbEskJApyKZlbbvE3QT/4AJo3h02bwq5IJOEoyKXkOnTwyxM3bfL9WT75JOyKRBKKglyC0bKl3zj03Xc+zPPzw65IJGEoyCU4jRvDP//pHxfXooXf3i8iEacgl2Cdf74P85o1oU0bmD077IpE4p6CXIJ3oNnWhRf6h1W8+mrYFYnENQW5REZamp8zb9YMbroJ/vKXsCsSiVsKcomck07yj477zW/g7rth8GA12xKJAAW5RFZKin+o8y23wJ/+BA88APv2hV2VSFxRrxWJvDJlYPRoOOUUv61/507429/UbEskIPqTJNGRlATPPAOpqdC/v2+2NXGimm2JBEBTKxI9Zn565fnnYfp0uOYa+M9/wq5KJOYpyCX6evaEV16Bt9+GVq2goCDsikRimoJcwnHTTTBtGqxZA5mZ8OWXYVckErMU5BKe9u1h7lzYvNmvN//447ArEolJJQ5yMzvDzBaZ2UdmtsbM7guiMEkQmZmweDHs3u3b4K5YEXZFIjEniCvyvcCDzrl6QFPgbjO7MIBxJVFcconvz5KS4rsoLlkSdkUiMaXEQe6c2+KcW7H/413AR0Ctko4rCea88yA3F2rVguxsmDkz7IpEYkagc+RmVhu4BFh6hK/1MLM8M8sr0CoFOZL0dN9sq359uO46v7JFRI4rsCA3s8rAFKCXc+6wxcHOuRHOuQznXEZaWlpQp5V4U706LFzo585vvhmeey7sikRKvUCC3MzK4kN8gnPujSDGlARWpQq8+SZce61/wPPAgWq2JXIMQaxaMWAU8JFz7umSlySC37r/+utw663w6KM+0NVsS+SIgui10gy4GVhtZgce1PhH59ybAYwtiaxMGRg1CqpVg6ef9s22Xn4ZypYNuzKRUqXEQe6c+ydgAdQicrikJHjqKd9s65FHfLOtyZP9UkURAbSzU2KBGfzxj/4pQ7Nn++WJ334bdlUipYaCXGLH738Pf/87vPsuXHklfP112BWJlAoKcokt3br5zULr1vkt/V98EXZFIqFTkEvsyc6G+fPhq698s61168KuSCRUCnKJTc2a+Z4sP/3kr8zz8sKuSCQ0CnKJXQ0a+GZblSv7OfNFi8KuSCQUCnKJbeec48P8zDPh6qv9I+REEoyCXGJfrVr+sXENGsD118PYsWFXJBJVCnKJD6mp8NZbforl1lth+PCwKxKJGgW5xI/KlWHWLOjUCe6/H/70JzXbkoSgIJf4Ur48TJoEd9wBgwdDz55qtiVxL4imWSKlS5kyMHKkb7aVk+ObbY0dq2ZbErcU5BKfzODJJ/3ced++vjfLa69BxYphVyYSOE2tSHz7wx/gpZdgzhxo29Z3TxSJMwpyiX89esDEibB0qV/V8tVXYVckEigFuSSG3/7WN9tavx6uuAI2bgy7IpHAKMglcbRtCwsWwLZtPszXrg27IpFAKMglsVx+ud8F+vPPvtnWsmVhVyRSYgpySTwXXeT7s5x8MmRlwcKFYVckUiKBBLmZvWxmX5vZh0GMJxJxdev6MK9d2zfbmjo17IpETlhQV+RjgOyAxhKJjtNP9z3NGzWCzp1h9OiwKxI5IYEEuXPubWBHEGOJRFW1av4G6FVXwe23w9NPh12RSLFFbY7czHqYWZ6Z5RUUFETrtCLHV6kSzJgBXbrAgw9Cv35qtiUxJWpb9J1zI4ARABkZGfpTIqVL+fLw6qtQtSoMGQLbt8Of/wzJyWFXJnJc6rUickByst/OX60aPPGE384/diyUKxd2ZSLHpCAXOZgZDB3qw/wPf/BhPmWKmm1JqRbU8sNXgf8DzjezTWZ2RxDjioTmoYd8K9x586B1a98KV6SUCuSK3Dl3QxDjiJQq3bv7OfMbb4SWLWHuXDjttLCrEjmMdnaKHEvnzjB7Nnz2me/P8q9/hV2RyGEU5CLH07q1X2u+Y4cP8zVrwq5I5BcU5CJF0bSpb7blHGRm+t7mIqWEglykqOrXh9xcOOUU32xrwYKwKxIBFOQixVOnjm+2VbcutGvnlyaKhExBLlJcp50GixdDRoZ/8tCoUWFXJAlOQS5yIk45xa8xb9PGL1PMyQm7IklgCnKRE1WpEkyfDl27+g1Effuq2ZaEQlv0RUqiXDmYMMFfoT/xhF+i+OKLarYlUaUgFymp5GT4y198f5bHH/f9WcaP9x0VRaJAQS4SBDPf/jY11fc0//ZbeOMNP/0iEmGaIxcJ0gMP+FUsCxb4HaE79OAsiTwFuUjQbr8dXnsNli+HFi1gy5awK5I4pyAXiYROneDNN32TrWbNfNMtkQhRkItESlYWLFzo58uvuAI++CDsiiROKchFIqlJE3jnHb+ypUULePfdsCuSOKQgF4m0Cy/0/VmqV/c3QOfODbsiiTMKcpFoqF3bh/l558FvfgOTJ4ddkcQRBblItNSoAYsWwWWXQbdu/pmgIgFQkItEU9Wqfmrl6quhRw947DH4/vuwq5IYF0iQm1m2mX1sZp+aWd8gxhSJWxUrwrRp/qHOAwb4trh33OGfQLRvX9jVSQwyV8JubWaWDKwHWgObgPeBG5xza4/2PRkZGS4vL69E5xWJec758B471m8g+u9//YMrbr4ZbrgB0tLCrlAioUoV32ztBJjZcudcxqGvB9FrpQnwqXNuw/4TTQSuBY4a5CKC78/SogW0aEGvlJfIX7gDtm6Fx3bCY1uBrWFXKBHQ8NraDJ9WO9AxgwjyWsCXB32+Cbjs0IPMrAfQA+DMM88M4LQicaRsWX8ztEYN+PFH2LFd0yzxqtpJgQ8ZRJDbEV47bL7GOTcCGAF+aiWA84rEjeHDD/6sPHB6SJVILAriZucm4IyDPk8HNgcwroiIFEEQQf4+cK6Z1TGzckA3YEYA44qISBGUeGrFObfXzHoCc4Fk4GXn3JoSVyYiIkUSyBOCnHNvAm8GMZaIiBSPdnaKiMQ4BbmISIxTkIuIxDgFuYhIjFOQi4jEOAW5iEiMU5CLiMQ4BbmISIxTkIuIxDgFuYhIjFOQi4jEOAW5iEiMU5CLiMQ4BbmISIxTkIuIxDgFuYhIjFOQi4jEOAW5iEiMU5CLiMS4EgW5mXUxszVmts/MMoIqSkREiq6kV+QfAp2AtwOoRURETkCZknyzc+4jADMLpprj6NUL8vOjciqRqGvYEIYPD7sKiUVRmyM3sx5mlmdmeQUFBdE6rYhI3DvuFbmZLQBOO8KXHnHOTS/qiZxzI4ARABkZGa7IFR5EVysiIoc7bpA7566KRiEiInJitPxQRCTGlXT54XVmtgm4HJhtZnODKUtERIqqpKtWpgJTA6pFREROgKZWRERinIJcRCTGKchFRGKcglxEJMaZcye0N6dkJzUrAD6P+olLrjqwLewioijR3i/oPSeKWH3PZznn0g59MZQgj1VmluecS5guj4n2fkHvOVHE23vW1IqISIxTkIuIxDgFefGMCLuAKEu09wt6z4kirt6z5shFRGKcrshFRGKcglxEJMYpyE+AmfU2M2dm1cOuJdLMLMfM1pnZB2Y21cyqhl1TpJhZtseJuQIAAAIuSURBVJl9bGafmlnfsOuJNDM7w8wWmdlH+x+ifl/YNUWDmSWb2UozmxV2LUFRkBeTmZ0BtAa+CLuWKJkP1HfOXQysBx4OuZ6IMLNk4AXgauBC4AYzuzDcqiJuL/Cgc64e0BS4OwHeM8B9wEdhFxEkBXnxPQM8BCTEXWLn3Dzn3N79n74HpIdZTwQ1AT51zm1wzu0BJgLXhlxTRDnntjjnVuz/eBc+3GqFW1VkmVk60A74W9i1BElBXgxm1gH4t3NuVdi1hOR2YE7YRURILeDLgz7fRJyH2sHMrDZwCbA03Eoibjj+Qmxf2IUEqUQPlohHx3rYNPBHoE10K4q8ojxg28wewf9TfEI0a4siO8JrCfGvLjOrDEwBejnn/hN2PZFiZu2Br51zy82sZdj1BElBfoijPWzazC4C6gCrzAz8FMMKM2vinNsaxRIDd7wHbJvZ74D2QJaL340Hm4AzDvo8HdgcUi1RY2Zl8SE+wTn3Rtj1RFgzoIOZXQNUAE4ys1ecc/8Tcl0lpg1BJ8jMNgIZzrlY7KBWZGaWDTwNtHDOFYRdT6SYWRn8zdws4N/A+8CNzrk1oRYWQeavSMYCO5xzvcKuJ5r2X5H3ds61D7uWIGiOXI7nz0AVYL6Z5ZvZX8MuKBL239DtCczF3/SbHM8hvl8z4Gag1f7f2/z9V6sSY3RFLiIS43RFLiIS4xTkIiIxTkEuIhLjFOQiIjFOQS4iEuMU5CIiMU5BLiIS4/4fOVqg0Lm5D28AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def h(z):\n",
    "    return np.maximum(0, 1-z)\n",
    "\n",
    "x_axis = np.linspace(-5, 5)\n",
    "\n",
    "plt.plot(x_axis, h(x_axis), color='red', label = \"h(z) = max(0, 1-z)\")\n",
    "\n",
    "plt.plot([-5, 1], [-1, -1], color=\"blue\", label = \"derivative(h)\")\n",
    "plt.plot([1,5], [0, 0], color=\"blue\")\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We notice that h is derivative for all z except z=1. \n",
    "\n",
    "For $z!=1$, we have $\\delta h(z) = \\frac{\\delta h}{\\delta z}.$\n",
    "\n",
    "For $z=1$, we notice that all the slopes between -1 and 0 suit.\n",
    "\n",
    "Thus, we conlcude that : \n",
    "\n",
    "$$\n",
    "\\\\ \\delta h(z) = \\left\\{\n",
    "    \\begin{array}{ll}\n",
    "        -1 &\\mbox{if} & z < 1 \\\\\n",
    "        [-1, 0] &\\mbox{if} &z = 1 \\\\\n",
    "        0 &\\mbox{if} &z>1 \n",
    "    \\end{array}\n",
    "\\right.\n",
    "$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2.3\n",
    "\n",
    "Let's set : $$\n",
    "f(v,a) = \\frac{1}{2}  \\sum_{j = 1}^{m} v_{j}^{2} + c\\sum_{j = 1}^{n} \\max(0, 1 - y_{i}(x_{i}^{T}v + a))\n",
    "$$\n",
    "\n",
    "We can write this function as a composition of a linear application M and two separable functions $N$ and $H$ such that : \n",
    "\n",
    "$$\n",
    "f(v,a) = N(v,a) + cH(M(v,a))\n",
    "$$\n",
    "\n",
    "According to the definition of a separable function, we have that : \n",
    "\n",
    "$$\n",
    "N(w) = \\sum_{j = 1}^{m} N_{j}(w_j) \\space \\space where \\space \\space w=(v,a)\\\\\n",
    "and \\space \\space N_j = \\frac{1}{2} v_{j}^{2}.\n",
    "$$\n",
    "\n",
    "Therefore : $N:(v,a)\\rightarrow \\frac{1}{2}  \\sum_{j = 1}^{m} v_{j}^{2}$.\n",
    "\n",
    "Using the same reasoning, we get that : \n",
    "\n",
    "$\n",
    "M = \n",
    "\\begin{pmatrix}\n",
    "y_1 &   &    \\\\\n",
    "      &...&\\\\\n",
    "    &    &  y_n\n",
    "\\end{pmatrix}\n",
    "$ x \n",
    "$ \\begin{pmatrix}\n",
    "    X^{T},\n",
    "\\begin{pmatrix}\n",
    "1\\\\\n",
    "..\\\\\n",
    "1\n",
    "\\end{pmatrix}\n",
    "\\end{pmatrix} $thereore we get : \n",
    "$\n",
    "M(v,a)\\rightarrow \\begin{pmatrix} y_{0}(x_{0}^{T}v + a) & ... & y_{0}(x_{0}^{T}v + a) \\end{pmatrix}\n",
    "$\n",
    "\n",
    "and $H:(x)\\rightarrow \\sum_{i = 1}^{n} h(x_i)$, with $h(m) = max(0, 1-m). $\n",
    "\n",
    "\n",
    "Let's define $f : \\mathbb{R}^{m}$ x $\\mathbb{R} \\rightarrow \\mathbb{R}$ and $g : \\mathbb{R}^{n} \\rightarrow \\mathbb{R}$.\n",
    "M is a linear operator $\\mathbb{R}^{m}$ x $\\mathbb{R}$ x $\\mathbb{R}^{n} \\rightarrow \\mathbb{R}^{n}$ \n",
    "\n",
    "\n",
    "\n",
    "The $\\textbf{proposition 4.2}$ states that if f and g be two convex functions, and M a linear operator.:\n",
    "$$\n",
    "\\delta f(x) + M^{T}\\delta g(x) \\subseteq \\delta ( f + goM)(x)\n",
    "$$\n",
    "\n",
    "\n",
    "Because we have that $dom(g) = \\mathbb{R}$, then according to the course, we can conclude that : \n",
    "\n",
    "$$\n",
    "\\delta f(v,a) = \\delta(N(v,a)) + cM^{T}\\delta M(v,a)\n",
    "$$\n",
    "\n",
    "Therefore :  \n",
    "\n",
    "$$\n",
    "\\\\delta(N(v,a)) = \\sum_{j = 1}^{m} v_j = (v,0)\n",
    "$$\n",
    "$$\n",
    "\\\\ \\delta M(v,a) = \\prod_{k=1}^n \\delta h(M_i(v,a)) = \\prod_{k=1}^n \\delta h(y_i(x_i^{T}v + a))  \n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = 1\n",
    "x = np.concatenate([X,np.ones((569,1))], axis=1) \n",
    "## addition of a column of ones on the first column of X\n",
    "\n",
    "M = np.diag(Y) @ x\n",
    "#the entry variable is a line vector whose first columns correspond to v, and the last one to a\n",
    "\n",
    "def N(va):\n",
    "    return 0.5 * np.sum(va[:-1]**2)\n",
    "\n",
    "def Hm(va):\n",
    "    return np.sum(h(M @ va))\n",
    "\n",
    "def f(va):\n",
    "    return N(va) + c*Hm(va)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delta_N(va):\n",
    "    return np.concatenate([va[:-1], [0]])\n",
    "\n",
    "def delta_Hm(va):\n",
    "    return np.dot(x.T, (np.dot(M,va)>=1)- 1)\n",
    "\n",
    "def delta_f(va):\n",
    "    return delta_N(va) + c * delta_Hm(va)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2.5 : Subgradient method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sub_grad_method(va0, N):\n",
    "    va_moy = np.zeros(va0.shape)\n",
    "    gamma_sum = 0\n",
    "    \n",
    "    for i in range(N):\n",
    "        gamma = 0.001/np.sqrt(i+1)\n",
    "        gamma_sum += gamma\n",
    "        va_moy += va0 * gamma\n",
    "        \n",
    "        va0 = va0 - gamma*delta_f(va0)\n",
    "    \n",
    "    return va_moy/gamma_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "569.0\n",
      "28.087767544895126\n"
     ]
    }
   ],
   "source": [
    "va0 = np.zeros((31,))\n",
    "print(f(va0))\n",
    "va_1 = sub_grad_method(va0, 100000)\n",
    "print(f(va_1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part III -  Stochastic gradient method\n",
    "\n",
    "Let $f_i(v,a) = \\frac{1}{2} \\sum_{j = 1}^{m} v_j^{2} + cn max(0, 1 - y_i(x_i^{T}v + a))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3.1\n",
    "\n",
    "$I$ is a random variable that follows a uniform law on {1,...,n}.\n",
    "\n",
    "We have that : \n",
    "$$\n",
    "\\mathbb{E}[f_{I}(v,a)] = \\mathbb{E}[\\frac{1}{2} \\sum_{i = 1}^{m} v_j^{2}] + cn\\mathbb{E}[\\sum_{i = 1}^{n} max(0, 1 - y_i(x_i^{T}v + a))] \n",
    "\\\\ = \\frac{1}{2} \\sum_{i = 1}^{m} v_j^{2} + cn\\frac{1}{n}\\sum_{i = 1}^{n} \\mathbb{E}[max(0, 1 - y_i(x_i^{T}v + a)]\n",
    "$$\n",
    "Because the random variable is $I$, and $v$ and $a$ are fixed, we can conclude that : \n",
    "$$\n",
    "\\mathbb{E}[f_{I}(v,a)] = \\frac{1}{2} \\sum_{i = 1}^{m} v_j^{2} + c\\sum_{i = 1}^{n} max(0, 1 - y_i(x_i^{T}v + a)\n",
    "\\\\ = f(v,a)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3.2\n",
    "\n",
    "The function $f_{i}(v,a) = N(v,a) + cn$  x  $ h(M_i(v,a))$, where \n",
    "$M_i = y_i \n",
    "\\begin{pmatrix}\n",
    "x_i^{T} & 1\n",
    "\\end{pmatrix}\n",
    "$\n",
    "\n",
    "Therefore, the subdifferential of the function is : \n",
    "$\n",
    "\\delta f_{i}(v,a) = \\delta N(v,0) + cn$  x  $ M_i^{T}\\delta h(M_i(v,a))\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3.3 : Stochastic gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "c=1\n",
    "\n",
    "def I_var():\n",
    "    return random.randint(0,n)\n",
    "\n",
    "def N(va):\n",
    "    sum = 0\n",
    "    for i in range(n-1):\n",
    "        sum+= va[i]\n",
    "    return sum/2        \n",
    "\n",
    "def M(i):\n",
    "    y_i = np.diag(Y)[i][i] #get the value of yi\n",
    "    x_i = X[:,i] #get the i-th column\n",
    "    X_i = np.concatenate([x_i,np.ones((569,1))], axis=1)\n",
    "    return y_i @ X_i.T \n",
    "\n",
    "def delta_fi(va, i):\n",
    "    m = M(i)\n",
    "    delta = N(va) + c*n*m.T*h(M(i))\n",
    "    \n",
    "    \n",
    "def stoch_grad_method(va0, N):\n",
    "    n = x.shape[0]\n",
    "    va_moy = np.zeros(va0.shape)\n",
    "    gamma_sum = 0\n",
    "    \n",
    "    for i in range(N):\n",
    "        I = np.random.randint(n)\n",
    "        gamma = 0.001/np.sqrt(i+1)\n",
    "        gamma_sum += gamma\n",
    "        va_moy += va0 * gamma\n",
    "        \n",
    "        va0 = va0 - gamma * delta_fi(va0,I)\n",
    "    \n",
    "    return va_moy/gamma_sum"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
